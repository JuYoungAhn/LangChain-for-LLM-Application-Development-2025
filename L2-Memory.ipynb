{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22643b33",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "## Outline\n",
    "- ConversationBufferMemory\n",
    "- ConversationBufferWindowMemory\n",
    "- ConversationTokenBufferMemory\n",
    "- ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d8948c",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64224484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T07:24:09.223404Z",
     "iopub.status.busy": "2025-12-16T07:24:09.223000Z",
     "iopub.status.idle": "2025-12-16T07:24:09.234225Z",
     "shell.execute_reply": "2025-12-16T07:24:09.233023Z",
     "shell.execute_reply.started": "2025-12-16T07:24:09.223369Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be64d03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T07:24:09.526830Z",
     "iopub.status.busy": "2025-12-16T07:24:09.526422Z",
     "iopub.status.idle": "2025-12-16T07:24:11.048236Z",
     "shell.execute_reply": "2025-12-16T07:24:11.046368Z",
     "shell.execute_reply.started": "2025-12-16T07:24:09.526765Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.chains import ConversationChain\n",
    "from langchain_classic.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcf8cdb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T07:24:12.286389Z",
     "iopub.status.busy": "2025-12-16T07:24:12.285645Z",
     "iopub.status.idle": "2025-12-16T07:24:12.907033Z",
     "shell.execute_reply": "2025-12-16T07:24:12.905358Z",
     "shell.execute_reply.started": "2025-12-16T07:24:12.286344Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_326015/2132126055.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/tmp/ipykernel_326015/2132126055.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use `langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True,\n",
    "    memory = memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91b903be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T07:24:13.947632Z",
     "iopub.status.busy": "2025-12-16T07:24:13.947225Z",
     "iopub.status.idle": "2025-12-16T07:24:14.956351Z",
     "shell.execute_reply": "2025-12-16T07:24:14.954864Z",
     "shell.execute_reply.started": "2025-12-16T07:24:13.947596Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Andrew\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Andrew! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9963721e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T07:24:34.860205Z",
     "iopub.status.busy": "2025-12-16T07:24:34.859534Z",
     "iopub.status.idle": "2025-12-16T07:24:35.477370Z",
     "shell.execute_reply": "2025-12-16T07:24:35.475820Z",
     "shell.execute_reply.started": "2025-12-16T07:24:34.860149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e77d0adb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T07:24:48.482134Z",
     "iopub.status.busy": "2025-12-16T07:24:48.481458Z",
     "iopub.status.idle": "2025-12-16T07:24:49.496974Z",
     "shell.execute_reply": "2025-12-16T07:24:49.495406Z",
     "shell.execute_reply.started": "2025-12-16T07:24:48.482094Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Andrew. Is there anything else you would like to know, Andrew?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c2bf9f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T07:24:53.007527Z",
     "iopub.status.busy": "2025-12-16T07:24:53.007116Z",
     "iopub.status.idle": "2025-12-16T07:24:53.013961Z",
     "shell.execute_reply": "2025-12-16T07:24:53.012524Z",
     "shell.execute_reply.started": "2025-12-16T07:24:53.007490Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Andrew\\nAI: Hello Andrew! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1+1 equals 2. Is there anything else you would like to know?\\nHuman: What is my name?\\nAI: Your name is Andrew. Is there anything else you would like to know, Andrew?\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b19fb0e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T07:25:01.434918Z",
     "iopub.status.busy": "2025-12-16T07:25:01.434487Z",
     "iopub.status.idle": "2025-12-16T07:25:01.439900Z",
     "shell.execute_reply": "2025-12-16T07:25:01.438458Z",
     "shell.execute_reply.started": "2025-12-16T07:25:01.434883Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6056fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e13a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97fab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4b10ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a539e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638c5862",
   "metadata": {},
   "source": [
    "## Memory\n",
    "Large Language Models are 'stateless'\n",
    "- Each transaction is independent \n",
    "\n",
    "Chatbots appear to have memory ny providing the full conversation as 'context'\n",
    "\n",
    "LangChain provides several kinds of 'memory' to store and accumulate the conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d366cf",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f210ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acc95800",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53f867a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9605b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b3130e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferWindowMemory(k=1) # just remember 1 conversational exchange\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53292aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7acfed",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d365a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4dad29d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.memory import ConversationTokenBufferMemory\n",
    "from langchain_openai import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65ad1561",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a4ef354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7c539",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8faf8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.memory import ConversationSummaryBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f7ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d085b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17384ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eabfb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lcel_intro",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ†• Modern Approach: LCEL (LangChain Expression Language)\n",
    "\n",
    "ìœ„ì˜ `ConversationChain`ì€ **deprecated** ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ í‘œì¤€ì¸ **LCEL**ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤.\n",
    "\n",
    "## LCELì˜ ì¥ì \n",
    "- âœ… ë” ëª…ì‹œì ì´ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì½”ë“œ\n",
    "- âœ… íƒ€ì… ì•ˆì „ì„±\n",
    "- âœ… ìŠ¤íŠ¸ë¦¬ë° ì§€ì›\n",
    "- âœ… ë” ë‚˜ì€ ì¡°í•©ì„± (composability)\n",
    "- âœ… ìë™ ë¹„ë™ê¸° ì§€ì›"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lcel_memory_basic",
   "metadata": {},
   "source": [
    "## LCEL with Message History\n",
    "\n",
    "`RunnableWithMessageHistory`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lcel_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lcel_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„¸ì…˜ë³„ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"ì„¸ì…˜ IDì— í•´ë‹¹í•˜ëŠ” ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¥¼ ë°˜í™˜\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lcel_prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ ì •ì˜ (ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ í¬í•¨)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Have a conversation with the human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "# Chain ìƒì„± (LCEL ë¬¸ë²•: | ì—°ì‚°ì)\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lcel_with_history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ Runnable ìƒì„±\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lcel_usage",
   "metadata": {},
   "source": [
    "### ì‚¬ìš© ì˜ˆì œ\n",
    "\n",
    "ë™ì¼í•œ `session_id`ë¥¼ ì‚¬ìš©í•˜ë©´ ëŒ€í™”ê°€ ê¸°ì–µë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lcel_example1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²« ë²ˆì§¸ ë©”ì‹œì§€\n",
    "response = chain_with_history.invoke(\n",
    "    {\"input\": \"Hi, my name is Andrew\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user123\"}}\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lcel_example2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‘ ë²ˆì§¸ ë©”ì‹œì§€ - ì´ë¦„ì„ ê¸°ì–µí•˜ëŠ”ì§€ í™•ì¸\n",
    "response = chain_with_history.invoke(\n",
    "    {\"input\": \"What is my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user123\"}}\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lcel_example3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„¸ ë²ˆì§¸ ë©”ì‹œì§€\n",
    "response = chain_with_history.invoke(\n",
    "    {\"input\": \"What is 1+1?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user123\"}}\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lcel_history_view",
   "metadata": {},
   "source": [
    "### ëŒ€í™” íˆìŠ¤í† ë¦¬ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lcel_view_history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥ëœ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ í™•ì¸\n",
    "session_history = store[\"user123\"]\n",
    "print(\"=== ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ===\")\n",
    "for message in session_history.messages:\n",
    "    print(f\"{message.type}: {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lcel_window",
   "metadata": {},
   "source": [
    "## LCEL with Window Memory\n",
    "\n",
    "ìµœê·¼ Nê°œì˜ ë©”ì‹œì§€ë§Œ ê¸°ì–µí•˜ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lcel_window_impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def get_windowed_history(session_id: str, window_size: int = 2) -> BaseChatMessageHistory:\n",
    "    \"\"\"ìµœê·¼ window_size ê°œì˜ ë©”ì‹œì§€ë§Œ ë°˜í™˜\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    \n",
    "    # ì „ì²´ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°\n",
    "    full_history = store[session_id]\n",
    "    \n",
    "    # ìµœê·¼ window_size * 2 ê°œë§Œ ìœ ì§€ (user + assistant ìŒ)\n",
    "    messages = full_history.messages\n",
    "    if len(messages) > window_size * 2:\n",
    "        # ìƒˆë¡œìš´ ChatMessageHistory ìƒì„±í•˜ì—¬ ìµœê·¼ ë©”ì‹œì§€ë§Œ í¬í•¨\n",
    "        windowed = ChatMessageHistory()\n",
    "        for msg in messages[-(window_size * 2):]:\n",
    "            windowed.add_message(msg)\n",
    "        return windowed\n",
    "    \n",
    "    return full_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lcel_comparison",
   "metadata": {},
   "source": [
    "## ğŸ’¡ Classic vs LCEL ë¹„êµ\n",
    "\n",
    "### Classic ë°©ì‹ (Deprecated)\n",
    "```python\n",
    "from langchain_classic.chains import ConversationChain\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "response = conversation.predict(input=\"Hi, my name is Andrew\")\n",
    "```\n",
    "\n",
    "### LCEL ë°©ì‹ (Modern, Recommended)\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "# ëª…ì‹œì ì¸ í”„ë¡¬í”„íŠ¸\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# LCEL ì²´ì¸ (íŒŒì´í”„ ì—°ì‚°ì)\n",
    "chain = prompt | llm\n",
    "\n",
    "# ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì¶”ê°€\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# ì„¸ì…˜ IDë¡œ ëŒ€í™” ê´€ë¦¬\n",
    "response = chain_with_history.invoke(\n",
    "    {\"input\": \"Hi, my name is Andrew\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "### LCELì˜ ì¶”ê°€ ê¸°ëŠ¥\n",
    "\n",
    "#### 1. ìŠ¤íŠ¸ë¦¬ë°\n",
    "```python\n",
    "for chunk in chain_with_history.stream(\n",
    "    {\"input\": \"Tell me a joke\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}}\n",
    "):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "```\n",
    "\n",
    "#### 2. ë°°ì¹˜ ì²˜ë¦¬\n",
    "```python\n",
    "responses = chain_with_history.batch([\n",
    "    {\"input\": \"What is 1+1?\"},\n",
    "    {\"input\": \"What is 2+2?\"},\n",
    "])\n",
    "```\n",
    "\n",
    "#### 3. ë¹„ë™ê¸°\n",
    "```python\n",
    "response = await chain_with_history.ainvoke(\n",
    "    {\"input\": \"Hello\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lcel_summary",
   "metadata": {},
   "source": [
    "## ğŸ“š Summary\n",
    "\n",
    "### Classic Chains (langchain_classic)\n",
    "- âŒ Deprecated (2024ë…„ ì´í›„)\n",
    "- âŒ ë¬µì‹œì ì¸ ë™ì‘ (\"magic\" ì´ ë§ìŒ)\n",
    "- âŒ ì œí•œì ì¸ ê¸°ëŠ¥\n",
    "- âœ… ê°„ë‹¨í•œ ì‚¬ìš©ë²• (ì´ˆë³´ìì—ê²Œ ì¹œìˆ™)\n",
    "\n",
    "### LCEL (LangChain Expression Language)\n",
    "- âœ… í˜„ì¬ í‘œì¤€ (2024ë…„~)\n",
    "- âœ… ëª…ì‹œì ì´ê³  íˆ¬ëª…í•œ ì½”ë“œ\n",
    "- âœ… íŒŒì´í”„ ì—°ì‚°ì (`|`)ë¡œ ì§ê´€ì ì¸ ì²´ì´ë‹\n",
    "- âœ… ìŠ¤íŠ¸ë¦¬ë°, ë°°ì¹˜, ë¹„ë™ê¸° ìë™ ì§€ì›\n",
    "- âœ… íƒ€ì… ì•ˆì „ì„±\n",
    "- âœ… ë” ë‚˜ì€ ë””ë²„ê¹…\n",
    "- âœ… ë³µì¡í•œ ì²´ì¸ êµ¬ì„± ê°€ëŠ¥\n",
    "\n",
    "### ê¶Œì¥ì‚¬í•­\n",
    "- ìƒˆ í”„ë¡œì íŠ¸: **ë¬´ì¡°ê±´ LCEL ì‚¬ìš©**\n",
    "- ê¸°ì¡´ í”„ë¡œì íŠ¸: ê°€ëŠ¥í•˜ë©´ LCELë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜\n",
    "- í•™ìŠµ ëª©ì : ë‘˜ ë‹¤ ì´í•´í•˜ë©´ ì¢‹ì§€ë§Œ, LCEL ìš°ì„  í•™ìŠµ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (openai310)",
   "language": "python",
   "name": "openai310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}