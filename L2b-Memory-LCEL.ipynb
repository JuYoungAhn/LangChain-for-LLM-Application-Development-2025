{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üÜï LangChain Memory with LCEL (Modern Approach)\n",
    "\n",
    "This notebook demonstrates memory management using **LCEL (LangChain Expression Language)**.\n",
    "\n",
    "## What is LCEL?\n",
    "\n",
    "LCEL is LangChain's **new standard approach** (2024~)\n",
    "\n",
    "### Classic Chains vs LCEL\n",
    "\n",
    "| Feature | Classic Chains | LCEL |\n",
    "|---------|---------------|------|\n",
    "| Status | ‚ùå Deprecated | ‚úÖ Current Standard |\n",
    "| Explicitness | ‚ùå Implicit (\"magic\") | ‚úÖ Explicit |\n",
    "| Streaming | ‚ùå Limited | ‚úÖ Auto-supported |\n",
    "| Async | ‚ùå Manual | ‚úÖ Auto-supported |\n",
    "| Batch | ‚ùå Limited | ‚úÖ Auto-supported |\n",
    "| Type Safety | ‚ùå Weak | ‚úÖ Strong |\n",
    "| Debugging | ‚ùå Difficult | ‚úÖ Easy |\n",
    "\n",
    "### LCEL Core: Pipe Operator (`|`)\n",
    "\n",
    "```python\n",
    "# Classic\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# LCEL - More intuitive!\n",
    "chain = prompt | llm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic_lcel",
   "metadata": {},
   "source": [
    "## 1. Basic LCEL Chain (without memory)\n",
    "\n",
    "Let's start by creating a simple LCEL chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Answer this question: {question}\"\n",
    ")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "# Output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# LCEL chain: connect with pipe (|) operator!\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "print(\"Chain created!\")\n",
    "print(f\"Type: {type(chain)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_invoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the chain\n",
    "response = chain.invoke({\"question\": \"What is 1+1?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory_intro",
   "metadata": {},
   "source": [
    "## 2. LCEL with Message History (adding memory)\n",
    "\n",
    "Now let's add conversation history!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "session_store",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store message history per session\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"Returns message history for the given session ID.\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "print(\"Session history function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory_prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt with message history\n",
    "prompt_with_history = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Have a conversation with the human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # üëà Conversation history goes here\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "print(\"Prompt with history created!\")\n",
    "print(\"\\nPrompt structure:\")\n",
    "print(prompt_with_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory_chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LCEL chain\n",
    "chain = prompt_with_history | llm\n",
    "\n",
    "# Wrap with Runnable that automatically manages message history\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Chain with message history created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_memory",
   "metadata": {},
   "source": [
    "### Conversation Test - Remembering Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First message - introduce name\n",
    "response = chain_with_history.invoke(\n",
    "    {\"input\": \"Hi, my name is Andrew\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user123\"}}\n",
    ")\n",
    "print(\"AI:\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second message - check if name is remembered\n",
    "response = chain_with_history.invoke(\n",
    "    {\"input\": \"What is my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user123\"}}  # üëà Same session_id!\n",
    ")\n",
    "print(\"AI:\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third message\n",
    "response = chain_with_history.invoke(\n",
    "    {\"input\": \"What is 1+1?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user123\"}}\n",
    ")\n",
    "print(\"AI:\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth message - ask name again\n",
    "response = chain_with_history.invoke(\n",
    "    {\"input\": \"What was my name again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user123\"}}\n",
    ")\n",
    "print(\"AI:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "view_history",
   "metadata": {},
   "source": [
    "### View Saved Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show_history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View saved message history\n",
    "session_history = store[\"user123\"]\n",
    "print(\"=== Full Conversation History ===\")\n",
    "print(f\"Total messages: {len(session_history.messages)}\\n\")\n",
    "\n",
    "for i, message in enumerate(session_history.messages, 1):\n",
    "    role = \"üë§ Human\" if message.type == \"human\" else \"ü§ñ AI\"\n",
    "    print(f\"{i}. {role}: {message.content}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_session",
   "metadata": {},
   "source": [
    "### New Session - Won't Remember Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_session_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use different session_id\n",
    "response = chain_with_history.invoke(\n",
    "    {\"input\": \"What is my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user456\"}}  # üëà Different session_id!\n",
    ")\n",
    "print(\"AI (new session):\", response.content)\n",
    "print(\"\\n‚úÖ Different sessions don't share conversation history!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming",
   "metadata": {},
   "source": [
    "## 3. üöÄ Powerful LCEL Features\n",
    "\n",
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stream_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AI (streaming): \", end=\"\", flush=True)\n",
    "\n",
    "for chunk in chain_with_history.stream(\n",
    "    {\"input\": \"Tell me a short joke\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user123\"}}\n",
    "):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n‚úÖ Streaming allows receiving responses in real-time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch",
   "metadata": {},
   "source": [
    "### Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple questions at once\n",
    "questions = [\n",
    "    {\"input\": \"What is 2+2?\"},\n",
    "    {\"input\": \"What is 3+3?\"},\n",
    "    {\"input\": \"What is 4+4?\"},\n",
    "]\n",
    "\n",
    "responses = chain_with_history.batch(\n",
    "    questions,\n",
    "    config={\"configurable\": {\"session_id\": \"batch_test\"}}\n",
    ")\n",
    "\n",
    "print(\"=== Batch Processing Results ===\")\n",
    "for q, r in zip(questions, responses):\n",
    "    print(f\"Q: {q['input']}\")\n",
    "    print(f\"A: {r.content}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "async",
   "metadata": {},
   "source": [
    "### Async Processing\n",
    "\n",
    "LCEL automatically supports async!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "async_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def async_example():\n",
    "    response = await chain_with_history.ainvoke(\n",
    "        {\"input\": \"Hello from async!\"},\n",
    "        config={\"configurable\": {\"session_id\": \"async_test\"}}\n",
    "    )\n",
    "    return response.content\n",
    "\n",
    "# Run in Jupyter\n",
    "result = await async_example()\n",
    "print(\"Async result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "window_memory",
   "metadata": {},
   "source": [
    "## 4. Window Memory (Remember Only Recent N Messages)\n",
    "\n",
    "Implementing Classic's `ConversationBufferWindowMemory` with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "window_impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class WindowedChatHistory(ChatMessageHistory):\n",
    "    \"\"\"History that keeps only the most recent N messages\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size: int = 4):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "    \n",
    "    @property\n",
    "    def messages(self) -> List[BaseMessage]:\n",
    "        \"\"\"Returns only the most recent window_size messages\"\"\"\n",
    "        all_messages = super().messages\n",
    "        return all_messages[-self.window_size:] if len(all_messages) > self.window_size else all_messages\n",
    "\n",
    "# Window history store\n",
    "window_store = {}\n",
    "\n",
    "def get_windowed_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in window_store:\n",
    "        window_store[session_id] = WindowedChatHistory(window_size=2)  # Keep only 2 recent\n",
    "    return window_store[session_id]\n",
    "\n",
    "# Chain using window memory\n",
    "chain_with_window = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_windowed_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Window memory chain created (k=2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "window_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send multiple messages\n",
    "session = \"window_test\"\n",
    "\n",
    "messages = [\n",
    "    \"My name is Alice\",\n",
    "    \"I like pizza\",\n",
    "    \"I live in Seoul\",\n",
    "    \"What do you remember about me?\"\n",
    "]\n",
    "\n",
    "for msg in messages:\n",
    "    response = chain_with_window.invoke(\n",
    "        {\"input\": msg},\n",
    "        config={\"configurable\": {\"session_id\": session}}\n",
    "    )\n",
    "    print(f\"User: {msg}\")\n",
    "    print(f\"AI: {response.content}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nüí° With window size of 2, the first message (name) should be forgotten!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "## 5. üìä Classic vs LCEL Comparison\n",
    "\n",
    "### Classic Approach (Deprecated)\n",
    "\n",
    "```python\n",
    "from langchain_classic.chains import ConversationChain\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "# ‚ùå Implicit behavior\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ‚ùå No session management\n",
    "# ‚ùå Limited streaming\n",
    "# ‚ùå Difficult batch processing\n",
    "response = conversation.predict(input=\"Hi, my name is Andrew\")\n",
    "```\n",
    "\n",
    "### LCEL Approach (Current Standard)\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "# ‚úÖ Explicit prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# ‚úÖ Clear chain composition\n",
    "chain = prompt | llm\n",
    "\n",
    "# ‚úÖ Session-based history management\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# ‚úÖ Supports invoke, stream, batch, ainvoke\n",
    "response = chain_with_history.invoke(\n",
    "    {\"input\": \"Hi, my name is Andrew\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benefits",
   "metadata": {},
   "source": [
    "## üéØ Why Use LCEL?\n",
    "\n",
    "### 1. Explicitness\n",
    "- Code clearly shows what's happening\n",
    "- No hidden \"magic\" behavior\n",
    "\n",
    "### 2. Composability\n",
    "```python\n",
    "# Easy to compose chains\n",
    "chain1 = prompt1 | llm\n",
    "chain2 = prompt2 | llm\n",
    "combined = chain1 | transform | chain2\n",
    "```\n",
    "\n",
    "### 3. Automatic Features\n",
    "- ‚úÖ Streaming\n",
    "- ‚úÖ Batch\n",
    "- ‚úÖ Async\n",
    "- ‚úÖ Parallel execution\n",
    "- ‚úÖ Fallbacks\n",
    "- ‚úÖ Retries\n",
    "\n",
    "### 4. Debugging\n",
    "```python\n",
    "# Perfect integration with LangSmith\n",
    "# Clear tracking of each step\n",
    "```\n",
    "\n",
    "### 5. Type Safety\n",
    "```python\n",
    "# IDE autocomplete\n",
    "# Type checking\n",
    "# Fewer runtime errors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## üìö Summary\n",
    "\n",
    "### Classic Chains (L2-Memory.ipynb)\n",
    "- ‚ùå **Deprecated** (since 2024)\n",
    "- ‚ùå Implicit behavior\n",
    "- ‚ùå Limited features\n",
    "- ‚úÖ Simple to use (beginner-friendly)\n",
    "\n",
    "### LCEL (this notebook)\n",
    "- ‚úÖ **Current Standard** (2024~)\n",
    "- ‚úÖ Explicit and transparent code\n",
    "- ‚úÖ Intuitive with pipe operator (`|`)\n",
    "- ‚úÖ Auto-support for streaming, batch, async\n",
    "- ‚úÖ Type safety\n",
    "- ‚úÖ Better debugging\n",
    "- ‚úÖ Complex chain composition\n",
    "- ‚úÖ Session-based memory management\n",
    "\n",
    "### Recommendations\n",
    "1. **New projects**: Always use LCEL\n",
    "2. **Existing projects**: Migrate to LCEL when possible\n",
    "3. **Learning**: Understand both, but prioritize LCEL\n",
    "\n",
    "### Next Steps\n",
    "- L3-chains.ipynb: Build complex chains with LCEL\n",
    "- L4-QnA.ipynb: Implement RAG with LCEL\n",
    "- [LangChain LCEL Documentation](https://python.langchain.com/docs/expression_language/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}